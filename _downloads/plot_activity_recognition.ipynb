{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nActivity recognition from accelerometer data\n============================================\n\nThis demo shows how the **sklearn-xarray** package works with the ``Pipeline``\nand ``GridSearchCV`` methods from scikit-learn providing a metadata-aware\ngrid-searchable pipeline mechansism.\n\nThe package combines the metadata-handling capabilities of xarray with the\nmachine-learning framework of sklearn. It enables the user to apply\npreprocessing steps group by group, use transformers that change the number\nof samples, use metadata directly as labels for classification tasks and more.\n\nThe example performs activity recognition from raw accelerometer data with a\nfeedforward neural network. It uses the `WISDM activity prediction dataset`_\nwhich contains the activities\nwalking, jogging, walking upstairs, walking downstairs, sitting and standing\nfrom 36 different subjects.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from __future__ import print_function\n\nimport numpy as np\n\nfrom sklearn_xarray import wrap, Target\nfrom sklearn_xarray.preprocessing import (Splitter, Sanitizer, Featurizer)\nfrom sklearn_xarray.model_selection import CrossValidatorWrapper\nfrom sklearn_xarray.datasets import load_wisdm_dataarray\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GroupShuffleSplit, GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "First, we load the dataset and plot an example of one subject performing\nthe 'Walking' activity.\n\n.. tip::\n\n    In the jupyter notebook version, change the first cell to ``%matplotlib\n    notebook`` in order to get an interactive plot that you can zoom and pan.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "X = load_wisdm_dataarray()\n\nX_plot = X[np.logical_and(X.activity == 'Walking', X.subject == 1)]\nX_plot = X_plot[:500] / 9.81\nX_plot['sample'] = (X_plot.sample - X_plot.sample[0]) / np.timedelta64(1, 's')\n\nf, axarr = plt.subplots(3, 1, sharex=True)\n\naxarr[0].plot(X_plot.sample, X_plot.sel(axis='x'), color='#1f77b4')\naxarr[0].set_title('Acceleration along x-axis')\n\naxarr[1].plot(X_plot.sample, X_plot.sel(axis='y'), color='#ff7f0e')\naxarr[1].set_ylabel('Acceleration [g]')\naxarr[1].set_title('Acceleration along y-axis')\n\naxarr[2].plot(X_plot.sample, X_plot.sel(axis='z'), color='#2ca02c')\naxarr[2].set_xlabel('Time [s]')\naxarr[2].set_title('Acceleration along z-axis')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Then we define a pipeline with various preprocessing steps and a classifier.\n\nThe preprocessing consists of splitting the data into segments, removing\nsegments with `nan` values and standardizing. Since the accelerometer data is\nthree-dimensional but the standardizer and classifier expect a one-dimensional\nfeature vector, we have to vectorize the samples.\n\nFinally, we use PCA and a naive Bayes classifier for classification.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "pl = Pipeline([\n    ('splitter', Splitter(\n        groupby=['subject', 'activity'], new_dim='timepoint', new_len=30)),\n    ('sanitizer', Sanitizer()),\n    ('featurizer', Featurizer()),\n    ('scaler', wrap(StandardScaler)),\n    ('pca', wrap(PCA, reshapes='feature')),\n    ('cls', wrap(GaussianNB, reshapes='feature'))\n])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Since we want to use cross-validated grid search to find the best model\nparameters, we define a cross-validator. In order to make sure the model\nperforms subject-independent recognition, we use a `GroupShuffleSplit`\ncross-validator that ensures that the same subject will not appear in both\ntraining and validation set.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "cv = CrossValidatorWrapper(\n    GroupShuffleSplit(n_splits=2, test_size=0.5), groupby=['subject'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The grid search will try different numbers of PCA components to find the best\nparameters for this task.\n\n.. tip::\n\n    To use multi-processing, set ``n_jobs=-1``.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "gs = GridSearchCV(\n    pl, cv=cv, n_jobs=1, verbose=1, param_grid={\n        'pca__n_components': [10, 20]\n    })"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The label to classify is the activity which we convert to an integer\nrepresentation for the classification.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "y = Target(coord='activity',\n           transform_func=LabelEncoder().fit_transform,\n           dim='sample')(X)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Finally, we run the grid search and print out the best parameter combination.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "if __name__ == '__main__':  # in order for n_jobs=-1 to work on Windows\n    gs.fit(X, y)\n    print('Best parameters: {0}'.format(gs.best_params_))\n    print('Accuracy: {0}'.format(gs.best_score_))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The performance of this classifier is obviously pretty bad,\n    it was chosen for execution speed, not accuracy.</p></div>\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.15", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}